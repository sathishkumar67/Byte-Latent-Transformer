{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1fdaa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Byte-Latent-Transformer'...\n",
      "remote: Enumerating objects: 124, done.\u001b[K\n",
      "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
      "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
      "remote: Total 124 (delta 68), reused 80 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (124/124), 32.94 KiB | 4.71 MiB/s, done.\n",
      "Resolving deltas: 100% (68/68), done.\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.2\n"
     ]
    }
   ],
   "source": [
    "# clone the BLT repository\n",
    "!git clone https://github.com/sathishkumar67/Byte-Latent-Transformer.git\n",
    "# move the files to the current directory\n",
    "!mv /kaggle/working/Byte-Latent-Transformer/* /kaggle/working/\n",
    "# upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# install latest version pytorch\n",
    "# install the required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f58180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from BLT.norms import RMSNorm\n",
    "from BLT.mlp import MLPwithSwiGLU\n",
    "from BLT.attention import MultiHeadLatentAttentionWithGQAFused\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EntropyConfig:\n",
    "\t# model hyperparameters\n",
    "\tvocab_size: int = 256\n",
    "\tnum_layers: int = 8\n",
    "\thead_bias: bool = False\n",
    "\n",
    "    # Attention hyperparameters\n",
    "\thidden_size: int = 512\n",
    "\tnum_heads: int = 8\n",
    "\tn_kv_heads: Optional[int] = None\n",
    "\tkv_lora_rank: int = 512\n",
    "\tqk_rope_head_dim: int = 64\n",
    "\tv_head_dim: int = 128\n",
    "\tqk_nope_head_dim: int = 128\n",
    "\tmax_position_embeddings: int = 2048\n",
    "\trope_base: int = 10000\n",
    "\tattn_dropout: float = 0.0\n",
    "\tattn_bias: bool = False\n",
    "\tuse_cache: bool = False\n",
    "\tis_causal: bool = True\n",
    "\n",
    "\t# MLP hyperparameters\n",
    "\tmlp_hidden_dim: Optional[int] = None  # If None, will be set to 4 * hidden_size\n",
    "\tmlp_dropout: float = 0.0\n",
    "\tmlp_bias: bool = False\n",
    "\n",
    "\t# RMSNorm hyperparameters\n",
    "\trmsnorm_eps: float = 1e-8\n",
    "\n",
    "\t# RotaryPositionEmbedding hyperparameters\n",
    "\trotary_max_position_embeddings: int = 2048\n",
    "\trotary_base: int = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "680f77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    EntropyBlock: Transformer block combining MultiHeadLatentAttentionWithGQAFused, RMSNorm, and MLPwithSwiGLU.\n",
    "\n",
    "    This block consists of:\n",
    "      - Pre-attention RMSNorm normalization\n",
    "      - Multi-head latent attention with grouped query and fused QKV projection\n",
    "      - Residual connection after attention\n",
    "      - Post-attention RMSNorm normalization\n",
    "      - Feed-forward MLP with SwiGLU activation\n",
    "      - Residual connection after MLP\n",
    "\n",
    "    Args:\n",
    "        config (EntropyConfig): Configuration dataclass containing all hyperparameters for attention, MLP, and normalization.\n",
    "\n",
    "    Attributes:\n",
    "        attention (MultiHeadLatentAttentionWithGQAFused): Multi-head latent attention module.\n",
    "        mlp (MLPwithSwiGLU): Feed-forward network with SwiGLU activation.\n",
    "        rmsnorm_1 (RMSNorm): RMSNorm layer before attention.\n",
    "        rmsnorm_2 (RMSNorm): RMSNorm layer before MLP.\n",
    "        config (EntropyConfig): Configuration object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: EntropyConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EntropyBlock.\n",
    "\n",
    "        Sets up attention, MLP, and normalization layers using the provided configuration.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Multi-head latent attention with grouped query and fused QKV projection\n",
    "        self.attention = MultiHeadLatentAttentionWithGQAFused(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads,\n",
    "            n_kv_heads=config.n_kv_heads,\n",
    "            kv_lora_rank=config.kv_lora_rank,\n",
    "            qk_rope_head_dim=config.qk_rope_head_dim,\n",
    "            v_head_dim=config.v_head_dim,\n",
    "            qk_nope_head_dim=config.qk_nope_head_dim,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            rope_base=config.rope_base,\n",
    "            attn_dropout=config.attn_dropout,\n",
    "            attn_bias=config.attn_bias\n",
    "        )\n",
    "\n",
    "        # Feed-forward network with SwiGLU activation\n",
    "        self.mlp = MLPwithSwiGLU(\n",
    "            dim=config.hidden_size,\n",
    "            hidden_dim=config.mlp_hidden_dim,\n",
    "            mlp_dropout=config.mlp_dropout,\n",
    "            mlp_bias=config.mlp_bias\n",
    "        )\n",
    "\n",
    "        # RMSNorm layers for pre-attention and pre-MLP normalization\n",
    "        self.rmsnorm_1 = RMSNorm(\n",
    "            dim=config.hidden_size,\n",
    "            eps=config.rmsnorm_eps\n",
    "        )\n",
    "        self.rmsnorm_2 = RMSNorm(\n",
    "            dim=config.hidden_size,\n",
    "            eps=config.rmsnorm_eps\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for EntropyBlock.\n",
    "\n",
    "        Applies RMSNorm, attention, and MLP with residual connections.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (Optional[torch.Tensor]): Optional attention mask for attention module.\n",
    "            past_key_value (Optional[Tuple[torch.Tensor, torch.Tensor]]): Optional cached key/value states for attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [batch_size, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "        # Pre-attention normalization and attention block with residual connection\n",
    "        hidden_states = hidden_states + self.attention(\n",
    "            self.rmsnorm_1(hidden_states),\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_value=past_key_value\n",
    "        )\n",
    "\n",
    "        # Pre-MLP normalization and MLP block with residual connection\n",
    "        hidden_states = hidden_states + self.mlp(self.rmsnorm_2(hidden_states))\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64ef109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    EntropyModel: Transformer-based model for byte-level sequence modeling.\n",
    "\n",
    "    This model consists of:\n",
    "      - Token embedding layer for input tokens\n",
    "      - Multiple stacked EntropyBlock transformer blocks\n",
    "      - Final RMSNorm normalization\n",
    "      - Output linear layer (head) for logits\n",
    "      - Weight sharing between embedding and output head\n",
    "\n",
    "    Args:\n",
    "        config (EntropyConfig): Configuration dataclass containing all hyperparameters for embeddings, attention, MLP, normalization, and output.\n",
    "\n",
    "    Attributes:\n",
    "        config (EntropyConfig): Model configuration.\n",
    "        entropy_block (nn.ModuleDict): Contains token embedding, stacked blocks, and final RMSNorm.\n",
    "        head (nn.Linear): Output projection layer mapping hidden states to vocabulary logits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: EntropyConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EntropyModel.\n",
    "\n",
    "        Sets up token embedding, transformer blocks, final normalization, and output head.\n",
    "        Implements weight sharing between embedding and output head for parameter efficiency.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # ModuleDict for embedding, blocks, and final normalization\n",
    "        self.entropy_block = nn.ModuleDict(dict(\n",
    "            token_embedding = nn.Embedding(config.vocab_size, config.hidden_size),  # Token embedding\n",
    "            blocks = nn.ModuleList([EntropyBlock(config) for _ in range(config.num_layers)]),  # Stacked transformer blocks\n",
    "            rms_final = RMSNorm(config.hidden_size)  # Final normalization\n",
    "        ))\n",
    "\n",
    "        # Output projection layer (head) for logits\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.head_bias)\n",
    "\n",
    "        # Weight sharing: output head uses embedding weights\n",
    "        self.head.weight = self.entropy_block.token_embedding.weight\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs: torch.Tensor,\n",
    "            targets: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass for EntropyModel.\n",
    "\n",
    "        Processes input tokens through embedding, stacked transformer blocks, final normalization, and output head.\n",
    "        Optionally computes cross-entropy loss if targets are provided.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): Input tensor of token indices [batch_size, seq_len].\n",
    "            targets (Optional[torch.Tensor]): Target tensor for loss computation [batch_size, seq_len].\n",
    "            attention_mask (Optional[torch.Tensor]): Optional attention mask for transformer blocks.\n",
    "            past_key_value (Optional[Tuple[torch.Tensor, torch.Tensor]]): Optional cached key/value states for attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits of shape [batch_size, seq_len, vocab_size].\n",
    "            If targets is provided, also returns cross-entropy loss.\n",
    "        \"\"\"\n",
    "        _, T = inputs.shape\n",
    "\n",
    "        # Ensure input sequence length does not exceed maximum allowed\n",
    "        assert T <= self.config.max_position_embeddings, \"Input sequence length exceeds maximum position length\"\n",
    "\n",
    "        # Token embedding\n",
    "        x = self.entropy_block.token_embedding(inputs)\n",
    "\n",
    "        # Pass through each transformer block\n",
    "        for block in self.entropy_block.blocks:\n",
    "            x = block(x, attention_mask=attention_mask, past_key_value=past_key_value)\n",
    "\n",
    "        # Final normalization and output projection\n",
    "        x = self.head(self.entropy_block.rms_final(x))\n",
    "\n",
    "        # If targets are provided, compute cross-entropy loss\n",
    "        if targets is not None:\n",
    "            # Flatten logits and targets for loss computation\n",
    "            return x, F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n",
    "        \n",
    "        # Otherwise, return logits only\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e53995ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EntropyConfig()\n",
    "model = EntropyModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "282ce29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 42.161408M\n"
     ]
    }
   ],
   "source": [
    "# count the number of parameters in the model\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params/1e6}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ccaea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
