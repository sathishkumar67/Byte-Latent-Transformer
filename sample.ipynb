{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1fdaa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Byte-Latent-Transformer'...\n",
      "remote: Enumerating objects: 124, done.\u001b[K\n",
      "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
      "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
      "remote: Total 124 (delta 68), reused 80 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (124/124), 32.94 KiB | 4.71 MiB/s, done.\n",
      "Resolving deltas: 100% (68/68), done.\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.2\n"
     ]
    }
   ],
   "source": [
    "# clone the BLT repository\n",
    "!git clone https://github.com/sathishkumar67/Byte-Latent-Transformer.git\n",
    "# move the files to the current directory\n",
    "!mv /kaggle/working/Byte-Latent-Transformer/* /kaggle/working/\n",
    "# upgrade pip\n",
    "!pip install --upgrade pip\n",
    "# install latest version pytorch\n",
    "# install the required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f58180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import inspect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from BLT.norms import RMSNorm\n",
    "from BLT.mlp import MLPwithSwiGLU\n",
    "from BLT.attention import MultiHeadLatentAttentionWithGQAFused\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EntropyConfig:\n",
    "    # Attention hyperparameters\n",
    "\thidden_size: int = 512\n",
    "\tnum_heads: int = 8\n",
    "\tn_kv_heads: Optional[int] = None\n",
    "\tkv_lora_rank: int = 512\n",
    "\tqk_rope_head_dim: int = 64\n",
    "\tv_head_dim: int = 128\n",
    "\tqk_nope_head_dim: int = 128\n",
    "\tmax_position_embeddings: int = 2048\n",
    "\trope_base: int = 10000\n",
    "\tattn_dropout: float = 0.0\n",
    "\tattn_bias: bool = False\n",
    "\tuse_cache: bool = False\n",
    "\tis_causal: bool = True\n",
    "\n",
    "\t# MLP hyperparameters\n",
    "\tmlp_hidden_dim: Optional[int] = None  # If None, will be set to 4 * hidden_size\n",
    "\tmlp_dropout: float = 0.0\n",
    "\tmlp_bias: bool = False\n",
    "\n",
    "\t# RMSNorm hyperparameters\n",
    "\trmsnorm_eps: float = 1e-8\n",
    "\n",
    "\t# RotaryPositionEmbedding hyperparameters\n",
    "\trotary_max_position_embeddings: int = 2048\n",
    "\trotary_base: int = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    EntropyBlock: Transformer block combining MultiHeadLatentAttentionWithGQAFused, RMSNorm, and MLPwithSwiGLU.\n",
    "\n",
    "    This block consists of:\n",
    "      - Pre-attention RMSNorm normalization\n",
    "      - Multi-head latent attention with grouped query and fused QKV projection\n",
    "      - Residual connection after attention\n",
    "      - Post-attention RMSNorm normalization\n",
    "      - Feed-forward MLP with SwiGLU activation\n",
    "      - Residual connection after MLP\n",
    "\n",
    "    Args:\n",
    "        config (EntropyConfig): Configuration dataclass containing all hyperparameters for attention, MLP, and normalization.\n",
    "\n",
    "    Attributes:\n",
    "        attention (MultiHeadLatentAttentionWithGQAFused): Multi-head latent attention module.\n",
    "        mlp (MLPwithSwiGLU): Feed-forward network with SwiGLU activation.\n",
    "        rmsnorm_1 (RMSNorm): RMSNorm layer before attention.\n",
    "        rmsnorm_2 (RMSNorm): RMSNorm layer before MLP.\n",
    "        config (EntropyConfig): Configuration object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: EntropyConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the EntropyBlock.\n",
    "\n",
    "        Sets up attention, MLP, and normalization layers using the provided configuration.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Multi-head latent attention with grouped query and fused QKV projection\n",
    "        self.attention = MultiHeadLatentAttentionWithGQAFused(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads,\n",
    "            n_kv_heads=config.n_kv_heads,\n",
    "            kv_lora_rank=config.kv_lora_rank,\n",
    "            qk_rope_head_dim=config.qk_rope_head_dim,\n",
    "            v_head_dim=config.v_head_dim,\n",
    "            qk_nope_head_dim=config.qk_nope_head_dim,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            rope_base=config.rope_base,\n",
    "            attn_dropout=config.attn_dropout,\n",
    "            attn_bias=config.attn_bias\n",
    "        )\n",
    "\n",
    "        # Feed-forward network with SwiGLU activation\n",
    "        self.mlp = MLPwithSwiGLU(\n",
    "            dim=config.hidden_size,\n",
    "            hidden_dim=config.mlp_hidden_dim,\n",
    "            mlp_dropout=config.mlp_dropout,\n",
    "            mlp_bias=config.mlp_bias\n",
    "        )\n",
    "\n",
    "        # RMSNorm layers for pre-attention and pre-MLP normalization\n",
    "        self.rmsnorm_1 = RMSNorm(\n",
    "            dim=config.hidden_size,\n",
    "            eps=config.rmsnorm_eps\n",
    "        )\n",
    "        self.rmsnorm_2 = RMSNorm(\n",
    "            dim=config.hidden_size,\n",
    "            eps=config.rmsnorm_eps\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for EntropyBlock.\n",
    "\n",
    "        Applies RMSNorm, attention, and MLP with residual connections.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): Input tensor of shape [batch_size, seq_len, hidden_size].\n",
    "            attention_mask (Optional[torch.Tensor]): Optional attention mask for attention module.\n",
    "            past_key_value (Optional[Tuple[torch.Tensor, torch.Tensor]]): Optional cached key/value states for attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [batch_size, seq_len, hidden_size].\n",
    "        \"\"\"\n",
    "        # Pre-attention normalization and attention block with residual connection\n",
    "        hidden_states = hidden_states + self.attention(\n",
    "            self.rmsnorm_1(hidden_states),\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_value=past_key_value\n",
    "        )\n",
    "\n",
    "        # Pre-MLP normalization and MLP block with residual connection\n",
    "        hidden_states = hidden_states + self.mlp(self.rmsnorm_2(hidden_states))\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyModel(nn.Module):\n",
    "    def __init__(self, config: EntropyConfig):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
